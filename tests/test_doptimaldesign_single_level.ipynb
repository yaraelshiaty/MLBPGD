{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import astra\n",
    "from scipy.optimize import newton, minimize\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "\n",
    "import pylops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.int = np.int32\n",
    "\n",
    "nx, ny = 128, 128  # Image size\n",
    "num_proj = 180  # Number of projections\n",
    "angles = np.linspace(0, np.pi, num_proj, endpoint=False)  # Projection angles\n",
    "\n",
    "geometry = astra.create_proj_geom('parallel', 1.0, 180, num_proj, angles)\n",
    "vol_geom = astra.create_vol_geom(nx, ny)\n",
    "proj_id = astra.create_projector('line', geometry, vol_geom)\n",
    "matrix_id = astra.projector.matrix(proj_id)\n",
    "\n",
    "\n",
    "H = astra.matrix.get(matrix_id)\n",
    "H = sp.csr_matrix(H)\n",
    "\n",
    "H = torch.sparse_csr_tensor(\n",
    "    torch.tensor(H.indptr, dtype=torch.int32),\n",
    "    torch.tensor(H.indices, dtype=torch.int32),\n",
    "    torch.tensor(H.data, dtype=torch.float32),\n",
    "    size=H.shape\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(crow_indices=tensor([    0,     1,     4,    10,    19,    31,    46,\n",
       "                               64,    84,   108,   135,   164,   196,   232,\n",
       "                              270,   311,   355,   402,   452,   505,   560,\n",
       "                              619,   681,   745,   812,   883,   956,  1032,\n",
       "                             1111,  1193,  1278,  1366,  1456,  1550,  1647,\n",
       "                             1746,  1848,  1954,  2062,  2173,  2287,  2404,\n",
       "                             2524,  2647,  2772,  2901,  3033,  3167,  3304,\n",
       "                             3445,  3588,  3734,  3883,  4036,  4191,  4349,\n",
       "                             4510,  4673,  4840,  5010,  5182,  5357,  5536,\n",
       "                             5717,  5901,  6088,  6278,  6471,  6667,  6865,\n",
       "                             7067,  7272,  7479,  7689,  7903,  8119,  8338,\n",
       "                             8560,  8784,  9008,  9231,  9454,  9678,  9902,\n",
       "                            10125, 10348, 10572, 10796, 11019, 11242, 11466,\n",
       "                            11690, 11913, 12136, 12360, 12584, 12807, 13030,\n",
       "                            13254, 13478, 13701, 13924, 14148, 14372, 14594,\n",
       "                            14813, 15029, 15243, 15453, 15660, 15865, 16067,\n",
       "                            16265, 16461, 16654, 16844, 17031, 17215, 17396,\n",
       "                            17575, 17750, 17922, 18092, 18259, 18422, 18583,\n",
       "                            18741, 18896, 19049, 19198, 19344, 19487, 19628,\n",
       "                            19765, 19899, 20031, 20160, 20285, 20408, 20528,\n",
       "                            20645, 20759, 20870, 20978, 21084, 21186, 21285,\n",
       "                            21382, 21476, 21566, 21654, 21739, 21821, 21900,\n",
       "                            21976, 22049, 22120, 22187, 22251, 22313, 22372,\n",
       "                            22427, 22480, 22530, 22577, 22621, 22662, 22700,\n",
       "                            22736, 22768, 22797, 22824, 22848, 22868, 22886,\n",
       "                            22901, 22913, 22922, 22928, 22931, 22932]),\n",
       "       col_indices=tensor([  127,   126,   127,  ..., 16256, 16257, 16256]),\n",
       "       values=tensor([0.1579, 0.9953, 0.6755,  ..., 0.6754, 0.9954, 0.1580]),\n",
       "       size=(180, 16384), nnz=22932, layout=torch.sparse_csr)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert SparseCsr layout tensor to numpy. Use Tensor.dense() first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(H\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtoarray(), cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m'\u001b[39m, aspect\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m, vmin \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, vmax \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.03\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mcolorbar()\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVisualization of Projection Matrix H\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert SparseCsr layout tensor to numpy. Use Tensor.dense() first."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.imshow(H.toarray(), cmap='gray', aspect='auto', vmin = 0, vmax = 0.03)\n",
    "plt.colorbar()\n",
    "plt.title(\"Visualization of Projection Matrix H\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(x):\n",
    "    X = torch.diag(x)\n",
    "    FIM = H @ X @ H.T\n",
    "    objective = torch.linalg.det(FIM)\n",
    "    log_padded = torch.where(objective > 1e-8, np.log(objective), 0)\n",
    "    return - log_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_obj(x, H):\n",
    "    H = H.to_sparse()\n",
    "    X = torch.diag(x)\n",
    "    HXHT = torch.sparse.mm(torch.sparse.mm(H,X),H.T)\n",
    "    \n",
    "    def solve_HXHT(b):\n",
    "        y = torch.linalg.solve(HXHT, b)  # Use torch's linear solver\n",
    "        return y\n",
    "\n",
    "    HXHT_inv_H = torch.stack([solve_HXHT(H[:, i]) for i in range(H.shape[1])], dim=1)\n",
    "    \n",
    "    # Compute diagonal elements efficiently\n",
    "    C_diag = torch.einsum(\"ij,ji->i\", H.T, HXHT_inv_H)\n",
    "    \n",
    "    return -C_diag\n",
    "\n",
    "def deriv_geom(x):\n",
    "    return -1 / x\n",
    "\n",
    "def newton_equation(theta, c, eq_constraint):\n",
    "    return torch.sum(1 / (c + theta)) - eq_constraint\n",
    "\n",
    "def newton_derivative(theta, c):\n",
    "    return -torch.sum(1 / (c + theta) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'aten::as_strided' with arguments from the 'SparseCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::as_strided' is only available for these backends: [CPU, Meta, QuantizedCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nCPU: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/build/aten/src/ATen/RegisterCPU.cpp:30455 [kernel]\nMeta: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/build/aten/src/ATen/RegisterMeta.cpp:26993 [kernel]\nQuantizedCPU: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/build/aten/src/ATen/RegisterQuantizedCPU.cpp:951 [kernel]\nBackendSelect: fallthrough registered at /opt/conda/conda-bld/pytorch_1724788960438/work/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/aten/src/ATen/functorch/DynamicLayer.cpp:497 [backend fallback]\nFunctionalize: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/build/aten/src/ATen/RegisterFunctionalization_0.cpp:22340 [kernel]\nNamed: fallthrough registered at /opt/conda/conda-bld/pytorch_1724788960438/work/aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: fallthrough registered at /opt/conda/conda-bld/pytorch_1724788960438/work/aten/src/ATen/ConjugateFallback.cpp:21 [kernel]\nNegative: fallthrough registered at /opt/conda/conda-bld/pytorch_1724788960438/work/aten/src/ATen/native/NegateFallback.cpp:22 [kernel]\nZeroTensor: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/build/aten/src/ATen/RegisterZeroTensor.cpp:161 [kernel]\nADInplaceOrView: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4942 [kernel]\nAutogradOther: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/torch/csrc/autograd/generated/VariableType_0.cpp:18032 [autograd kernel]\nAutogradCPU: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/torch/csrc/autograd/generated/VariableType_0.cpp:18032 [autograd kernel]\nAutogradCUDA: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/torch/csrc/autograd/generated/VariableType_0.cpp:18032 [autograd kernel]\nAutogradHIP: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/torch/csrc/autograd/generated/VariableType_0.cpp:18032 [autograd kernel]\nAutogradXLA: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/torch/csrc/autograd/generated/VariableType_0.cpp:18032 [autograd kernel]\nAutogradMPS: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/torch/csrc/autograd/generated/VariableType_0.cpp:18032 [autograd kernel]\nAutogradIPU: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/torch/csrc/autograd/generated/VariableType_0.cpp:18032 [autograd kernel]\nAutogradXPU: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/torch/csrc/autograd/generated/VariableType_0.cpp:18032 [autograd kernel]\nAutogradHPU: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/torch/csrc/autograd/generated/VariableType_0.cpp:18032 [autograd kernel]\nAutogradVE: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/torch/csrc/autograd/generated/VariableType_0.cpp:18032 [autograd kernel]\nAutogradLazy: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/torch/csrc/autograd/generated/VariableType_0.cpp:18032 [autograd kernel]\nAutogradMTIA: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/torch/csrc/autograd/generated/VariableType_0.cpp:18032 [autograd kernel]\nAutogradPrivateUse1: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/torch/csrc/autograd/generated/VariableType_0.cpp:18032 [autograd kernel]\nAutogradPrivateUse2: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/torch/csrc/autograd/generated/VariableType_0.cpp:18032 [autograd kernel]\nAutogradPrivateUse3: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/torch/csrc/autograd/generated/VariableType_0.cpp:18032 [autograd kernel]\nAutogradMeta: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/torch/csrc/autograd/generated/VariableType_0.cpp:18032 [autograd kernel]\nAutogradNestedTensor: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/torch/csrc/autograd/generated/VariableType_0.cpp:18032 [autograd kernel]\nTracer: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/torch/csrc/autograd/generated/TraceType_0.cpp:17004 [kernel]\nAutocastCPU: fallthrough registered at /opt/conda/conda-bld/pytorch_1724788960438/work/aten/src/ATen/autocast_mode.cpp:209 [backend fallback]\nAutocastXPU: fallthrough registered at /opt/conda/conda-bld/pytorch_1724788960438/work/aten/src/ATen/autocast_mode.cpp:351 [backend fallback]\nAutocastCUDA: fallthrough registered at /opt/conda/conda-bld/pytorch_1724788960438/work/aten/src/ATen/autocast_mode.cpp:165 [backend fallback]\nFuncTorchBatched: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:735 [kernel]\nBatchedNestedTensor: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /opt/conda/conda-bld/pytorch_1724788960438/work/aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/aten/src/ATen/LegacyBatchingRegistrations.cpp:1079 [kernel]\nVmapMode: fallthrough registered at /opt/conda/conda-bld/pytorch_1724788960438/work/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/aten/src/ATen/functorch/TensorWrapper.cpp:207 [backend fallback]\nPythonTLSSnapshot: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/aten/src/ATen/functorch/DynamicLayer.cpp:493 [backend fallback]\nPreDispatch: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(H\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m H\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m c \u001b[38;5;241m=\u001b[39m derivative_obj(x, H) \u001b[38;5;241m-\u001b[39m deriv_geom(x)\n\u001b[1;32m      3\u001b[0m theta \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m      5\u001b[0m theta_sol \u001b[38;5;241m=\u001b[39m newton(newton_equation, theta, fprime\u001b[38;5;241m=\u001b[39mnewton_derivative, args\u001b[38;5;241m=\u001b[39m(c,))\n",
      "Cell \u001b[0;32mIn[32], line 10\u001b[0m, in \u001b[0;36mderivative_obj\u001b[0;34m(x, H)\u001b[0m\n\u001b[1;32m      7\u001b[0m     y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39msolve(HXHT, b)  \u001b[38;5;66;03m# Use torch's linear solver\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\n\u001b[0;32m---> 10\u001b[0m HXHT_inv_H \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([solve_HXHT(H[:, i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(H\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Compute diagonal elements efficiently\u001b[39;00m\n\u001b[1;32m     13\u001b[0m C_diag \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mij,ji->i\u001b[39m\u001b[38;5;124m\"\u001b[39m, H\u001b[38;5;241m.\u001b[39mT, HXHT_inv_H)\n",
      "Cell \u001b[0;32mIn[32], line 10\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m     y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39msolve(HXHT, b)  \u001b[38;5;66;03m# Use torch's linear solver\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\n\u001b[0;32m---> 10\u001b[0m HXHT_inv_H \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([solve_HXHT(H[:, i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(H\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Compute diagonal elements efficiently\u001b[39;00m\n\u001b[1;32m     13\u001b[0m C_diag \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mij,ji->i\u001b[39m\u001b[38;5;124m\"\u001b[39m, H\u001b[38;5;241m.\u001b[39mT, HXHT_inv_H)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'aten::as_strided' with arguments from the 'SparseCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::as_strided' is only available for these backends: [CPU, Meta, QuantizedCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nCPU: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/build/aten/src/ATen/RegisterCPU.cpp:30455 [kernel]\nMeta: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/build/aten/src/ATen/RegisterMeta.cpp:26993 [kernel]\nQuantizedCPU: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/build/aten/src/ATen/RegisterQuantizedCPU.cpp:951 [kernel]\nBackendSelect: fallthrough registered at /opt/conda/conda-bld/pytorch_1724788960438/work/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/aten/src/ATen/functorch/DynamicLayer.cpp:497 [backend fallback]\nFunctionalize: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/build/aten/src/ATen/RegisterFunctionalization_0.cpp:22340 [kernel]\nNamed: fallthrough registered at /opt/conda/conda-bld/pytorch_1724788960438/work/aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: fallthrough registered at /opt/conda/conda-bld/pytorch_1724788960438/work/aten/src/ATen/ConjugateFallback.cpp:21 [kernel]\nNegative: fallthrough registered at /opt/conda/conda-bld/pytorch_1724788960438/work/aten/src/ATen/native/NegateFallback.cpp:22 [kernel]\nZeroTensor: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/build/aten/src/ATen/RegisterZeroTensor.cpp:161 [kernel]\nADInplaceOrView: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4942 [kernel]\nAutogradOther: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/torch/csrc/autograd/generated/VariableType_0.cpp:18032 [autograd kernel]\nAutogradCPU: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/torch/csrc/autograd/generated/VariableType_0.cpp:18032 [autograd kernel]\nAutogradCUDA: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/torch/csrc/autograd/generated/VariableType_0.cpp:18032 [autograd kernel]\nAutogradHIP: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/torch/csrc/autograd/generated/VariableType_0.cpp:18032 [autograd kernel]\nAutogradXLA: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/torch/csrc/autograd/generated/VariableType_0.cpp:18032 [autograd kernel]\nAutogradMPS: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/torch/csrc/autograd/generated/VariableType_0.cpp:18032 [autograd kernel]\nAutogradIPU: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/torch/csrc/autograd/generated/VariableType_0.cpp:18032 [autograd kernel]\nAutogradXPU: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/torch/csrc/autograd/generated/VariableType_0.cpp:18032 [autograd kernel]\nAutogradHPU: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/torch/csrc/autograd/generated/VariableType_0.cpp:18032 [autograd kernel]\nAutogradVE: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/torch/csrc/autograd/generated/VariableType_0.cpp:18032 [autograd kernel]\nAutogradLazy: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/torch/csrc/autograd/generated/VariableType_0.cpp:18032 [autograd kernel]\nAutogradMTIA: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/torch/csrc/autograd/generated/VariableType_0.cpp:18032 [autograd kernel]\nAutogradPrivateUse1: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/torch/csrc/autograd/generated/VariableType_0.cpp:18032 [autograd kernel]\nAutogradPrivateUse2: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/torch/csrc/autograd/generated/VariableType_0.cpp:18032 [autograd kernel]\nAutogradPrivateUse3: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/torch/csrc/autograd/generated/VariableType_0.cpp:18032 [autograd kernel]\nAutogradMeta: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/torch/csrc/autograd/generated/VariableType_0.cpp:18032 [autograd kernel]\nAutogradNestedTensor: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/torch/csrc/autograd/generated/VariableType_0.cpp:18032 [autograd kernel]\nTracer: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/torch/csrc/autograd/generated/TraceType_0.cpp:17004 [kernel]\nAutocastCPU: fallthrough registered at /opt/conda/conda-bld/pytorch_1724788960438/work/aten/src/ATen/autocast_mode.cpp:209 [backend fallback]\nAutocastXPU: fallthrough registered at /opt/conda/conda-bld/pytorch_1724788960438/work/aten/src/ATen/autocast_mode.cpp:351 [backend fallback]\nAutocastCUDA: fallthrough registered at /opt/conda/conda-bld/pytorch_1724788960438/work/aten/src/ATen/autocast_mode.cpp:165 [backend fallback]\nFuncTorchBatched: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:735 [kernel]\nBatchedNestedTensor: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /opt/conda/conda-bld/pytorch_1724788960438/work/aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/aten/src/ATen/LegacyBatchingRegistrations.cpp:1079 [kernel]\nVmapMode: fallthrough registered at /opt/conda/conda-bld/pytorch_1724788960438/work/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/aten/src/ATen/functorch/TensorWrapper.cpp:207 [backend fallback]\nPythonTLSSnapshot: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/aten/src/ATen/functorch/DynamicLayer.cpp:493 [backend fallback]\nPreDispatch: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at /opt/conda/conda-bld/pytorch_1724788960438/work/aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(H.shape[1]) / H.shape[1]\n",
    "c = derivative_obj(x, H) - deriv_geom(x)\n",
    "theta = 100\n",
    "\n",
    "theta_sol = newton(newton_equation, theta, fprime=newton_derivative, args=(c,))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLcupy_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
